



# 1. 基于微透镜阵列的计算成像研究

【以下内容摘自上篇文章中】

微透镜阵列能够实现多通道成像，并且体积小，重量轻，便于集成化。采集得到的图像数据量大小和常规方法得到的图片相当，在很大程度上方便了后期图像处理。

微透镜阵列对空间场景成像，可以看做为透镜对空间光线进行编码。通过计算成像方法对这些编码信息进行计算，就能够得到空间的三位信息。基于微透镜阵列的计算成像研究主要集中在光场相机成像和计算集成成像两个方面。

一个物体的外观需要由入射的照明条件，物体表面的几何特性，时间等因素同时决定。用光场这个概念来描述光在三维空间中的辐射传输特性，表示一根携带物体视觉特性的光线的传播规则。利用七维全光函数来表征光场。

![1551340904566](images/1551340904566.png)

光场成像由于记录的信息是空间的光场信息，观看时候需要对这些采集到的光场信息进行计算处理。光场采集与光场数据处理。光场采集方式的区别决定了光场信息处理方法的不同，按照结构的不同分为多相机组合和单相机改造。

通过对各个相机时间同步的精确控制以及相机间相对位置的精确计算，能够精确的对光场进行处理，获得高质量的合成图像。通过对所采集的光场进行处理，能够获得景深非常浅的图像，称为合成孔径成像。由于成像景深短，在还原被遮物体时候，能够使前方的离焦遮挡物体严重虚化，实现透视的效果。

![1551341582016](images/1551341582016.png)

## 1.1 光场成像

单相机的光场采集一般在相机内部加入某种编码调制原件，在成像过程中对光场信息进行调制，使相机内部的光场可以被二维光电传感器记录。EA等设计一种全光相机的原理结构。目标物体的信息，由主透镜进行采集，透过主透镜的光线经过微透镜阵列最终被探测器所采集。这些光线通过微透镜阵列形成若干个与微透镜数对应的宏像素。宏像素的意义在于坐标位置代表了目标像点的空间位置，宏像素所包含的探测器单元代表目标物体在不同视角情况下的成像细节。

![1551342408146](images/1551342408146.png)

为了让微透镜阵列更好的工作，系统还引入了一个光学扩散片与一个场景。光学扩散片箱单与低通滤波器，用来消除采样中的高频成分，这些高频成分高于微透镜阵列的采样频率，会在最终图像中造成噪声。场镜的主要作用是让为透镜所成的像能够更好的记录在探测器上。

Ｒ对全光相机进行了简化，微透镜阵列直接安装在探测芯片之前。减小了中继镜头带来的额外尺寸，实现了手持的光场相机。拍摄图像如下，可以看出每个微透镜都对应一个包含若干像元的宏像素。这些宏像素阵列记录了相机内部空间的四维光场矩阵。通过对嗣位光场的计算，可以获得聚焦在不同焦平面的图像。这种牺牲相机空间分辨率来获得角度分辨率，光场相机获得了常规相机不具备的重聚焦功能。

![1551342977553](images/1551342977553.png)

![1551342993872](images/1551342993872.png)

然而这种方法构建的光场相机在<font color=#ff0000>空间分辨率与角度分辨率与微透镜阵列的参数有直接的联系</font>，并且装调难度比较大。针对这个问题，提出了“聚焦型光场相机”，结构如下，光场相机2.0

![1551343184355](images/1551343184355.png)

改变微透镜阵列与主光学系统间的位置关系，使微透镜阵列对主光学系统所成实现进行二次成像。相机的空间分辨率和角度分辨率之间的权衡关系变成由微透镜阵列和主光学系统实像面距离来决定，这样就增加了系统空间分辨率选择的灵活性，来开了微透镜阵列与探测面之间的间隔，降低了光场相机对微透镜阵列的要求，方便了系统的装调。

---

小结：采用微透镜阵列进行光场成像的系统有两种方式，第一种是直接将微透镜阵列放在成像芯片之前，这样的系统会使成像的空间分辨率和角度分辨率和微透镜阵列的参数有之间的关系，因此不是特别的好。这里应该需要考虑微透镜阵列成像的时候物象大小和微透镜阵列之间的关系。为了避免上述的问题，光场相机2.0被提出来，也就是聚焦型光场相机，他是利用微透镜阵列对主光学系统得到的实像进行二次成像得到结果。通过改变微透镜和实像之间的距离可得到不同的空间分辨率角度分辨率。

---

## 1.2 计算集成成像

将采集设备放置在微透镜阵列的后焦面处，物体将透过透镜阵列成像在采集设备上，形成一个包含三维空间信息的单元图像阵列。在观察这个单元图像时，在其前面放置一个微透镜阵列即可观察到所采集物体的三维图案。当观察位置发生变化时，就可以观察到物体不同视角变化。

![1552617765928](images/1552617765928.png)

2001年，H提出任意视角的计算集成成像图片重建方法。利用微透镜阵列得到单元图像阵列后，通过视差计算，选择单元图像上具体位置处的像素组成该视点处的视点图像，该方法能够重建处任意视点处的视点图像，在立体测量上有非常大的应用潜力。

2004，S等人利用计算机对集成成像所采集到的图片进行逆向投影，成功重构处物体的三位图像。该方法利用集成成像的采集装置采集到包含空间信息的单元图阵列，利用计算机模拟处针孔阵列，透过这个针孔阵列将对应单元图像投影到任意的目标深度处，其系统结构与最终效果如图。

![1552618227435](images/1552618227435.png)

2008，Chunghong提出利用集成成像的视点图像来进行空间物体的深度测量，测距效果如图所示，通过柱透镜阵列获得单元图像阵列，结合多基线测距算法，比较准确的测量出空间物体的深度。

![1552618466684](images/1552618466684.png)

## 2.1 四维光场理论

传统相机中探测器的每个像素点记录的是到达该点的所有光线的叠加，不能对光线的入射信息进行表达。微透镜阵列同时记录了相机内部光线的强度和方向信息。

对全光函数进行简化，认为光在传播的过程中波长和强度都不随时间变化，以及光线的强度不会改变。函数简化为

![1552619920118](images/1552619920118.png)

函数的右边是四维坐标来表示的空间自由光线，左边是输出的光线强度。针对双平面参数化方法进行简单的介绍。该参数化方法利用空间光线和两个平行平面的焦点$(u,v)$和$(s,t)$来表示这条空间光线。该方法最大程度上减小奇异点，但是该方法不能表示平行于这两个平面的光线。

一维简化的相机内部空间光线传播如下图所示，一根独立的光线穿过光瞳面$u$，最终记录在探测器$x$上。这条光线在以$u$为纵轴，$x$为横轴的二维坐标系中可以表示成一个点。$u$和$x$决定了这条光线的方向信息，探测器像元的强度就是光线的强度。

![1552620842418](images/1552620842418.png)

空间中每条光线都对应于光场坐标分布图中的一个点。

![1552620908553](images/1552620908553.png)

对于传统相机中，一个物点发出的光线通过光瞳平面，汇聚在最终的像素上。由于传统相机结构限制，无法区分出光线与光瞳面的交点坐标，因此他的光场信息如图中所示。每一个长条代表了不同像素采集到的光线信息，长条的宽度等于像素像元的宽度。从图中可以看出常规相机实际上是采集到相机内部光场在$x$平面上的投影，这种投影最大化的保存$x$信息，但是完全丢失$u​$信息。

![1552637271026](images/1552637271026.png)

当聚焦物体的空间位置改变时，仍然以$x$作为四维光场的坐标面。当聚焦物体变近时，所成的像就要离焦平面更远一点。对应的光场分布如图，光场信息相对于正常情况发生了逆时针的倾斜。可以看出，光场分布图的倾斜情况正对应者相机聚焦位置的变化。

![1552638271279](images/1552638271279.png)

利用光场信息来建立探测面处的二维强度分布可以通过方程

![1552638545630](images/1552638545630.png)

---

小结：这里是聚焦物体发生位置变化时候出现的结果，整个光学系统内部的参数是不发生变化的。这里的$x,u$分别进行考虑，$x$可以看作是探测阵列上面的第几个探测像素元，而$u$可以看作是光瞳平面上第几根光线的位置。第一幅图中发生顺时针旋转，是因为$x,u$是正比的关系。第一个像元$x$对应第一个光线的光瞳面$u$。

---

## 2.2 斯坦福光场相机

光场相机克服了传统相机中光瞳面坐标信息丢失的问题。光场相机在主光学系统的像平面处添加了一块微透镜阵列，利用探测器来探测微透镜焦面处的光场图像。每个微透镜都对应于探测器上一小块区域，<font color=#ff0000>宏像素</font>。宏像素平面对应四维光场中$(s,t)$的坐标信息，每个宏像素都是对应微透镜对光瞳面所成的像。因此，每个宏像素下所包含的探测器像素都对应着光瞳面上的不同区域即$(u,v)$，通过对这些像素的重新计算，得到光场相机特有的一些三维功能。

![1552638987487](images/1552638987487.png)

每个像素不再是对应一个长条，而是光场分布图上的一个网格。这些网格表示入射到该宏像素处某一方向光线强度。光场相机中的条形宽度对应于宏像素的宽度，同一个宏像素记录的空间位置信息是相同的，只是角度不同。<font color=#ff0000>从这个角度来进行分析，光场相机牺牲了最终的空间分辨能力，即$x$的采样间隔，来获取角度分辨能力，即$u$的采样能力。</font>

拍照过程中的重聚焦可以对应于相机内部光场信息的倾斜，常规相机中一次曝光只能进行某个方向的投影，一次只能记录一个聚焦平面的信息。光场相机将空间的光场分布进行网格化，理论上只要进行合理的计算，就能得到重聚焦的图片。

![1552639627866](images/1552639627866.png)

当我们对光场所用的坐标系进行变换时，其所表示光线不会有所改变，只是表示所用的参考系发生了变换。

![1552640005533](images/1552640005533.png)

![1552640284186](images/1552640284186.png)

---

注意：上面的坐标变换中$L(u,v,s,t)$到$L^`(u^`,v^`,s^`,t^`)$只是利用两个式子变量间的集合关系，将左边的变量全部用右边的变量进行替换，而不存在其他的操作。对于一根光线来说，只要光学系统的内部结构不发生变化，那么光线的运行规律是不会发生变化的。

---

公式中$L$是以探测器面$(u,v)$和微透镜阵列面$(s,t)$为坐标系的空间光场。$I​$是我们在重聚焦面所得到的重聚焦图像。要获得最终的重聚焦图片，最直接的办法就是将我们采集到的光场图像按照这个方程进行计算，通过将坐标相同方向不同的光线叠加，得到最终的图像信息。

### 2.2.1 斯坦福光场相机仿真

利用zemax搭建了一套光场相机系统，仿真得到光学系统的光场图像，通过对仿真光场图片进行处理，验证光场相机的成像与重聚焦功能。

由主镜头、微透镜阵列和像平面组成，主光学系统采用理想透镜作为主光学系统。

![1552640986714](images/1552640986714.png)

光场采集过程中所用的光学系统中，光线透过理想主光学系统，成像在透镜阵列上。从光学系统的细节可以看出来，微透镜阵列将入射光线分散至最终的探测面上，实现对光线的记录。由于zemax只能针对平面物体进行成像，本文以软件自带的图像作为处理图进行光场相机成像仿真，为实现光场相机的重聚焦功能，本文将物体放在无穷远，微透镜阵列放在主光学系统焦平面后方，产生离焦效果。希望后期的光场处理中得到正确的聚焦图片。

原图和普通相机拍摄图如下所示

![1552641510881](images/1552641510881.png)

光场相机获得的光场图像如下所示，包含$200*270$个宏像素，部分放大细节中，看到每个宏像素都是一个圆形，每个宏像素下包含$10*10​$个像元。整体的图像轮廓中，只能分辨出小孩的离焦图片，从放大的宏像素细节下看出，每个宏像素都记录了部分的空间信息。

![1552641741268](images/1552641741268.png)

为了能够对采集的光场图像进行处理，需要将宏像素提取出来。观察发现宏像素不是按照固定的间隔进行排列，增加了宏像素提取时的难度。利用微透镜阵列的物像关系来确定宏像素中心位置，实现宏像素位置快速有效的确定。由于探测器放置在微透镜阵列的焦平面处，可以近似看作是对主光学系统光阑成像，物像关系确定下来。

---

注意：每一个宏像素都是对整个光学系统进行成像，但是由于微透镜阵列的中心位置的不同，所成的像也有一定程度上的差异。每个宏像素下的图像拉伸的程度不一样，可以根据透镜中心位置的关系，判断光学系统中心在宏像素处的位置，从而得知宏像素的拉伸结果。由于角度的问题，靠近微透镜中心处的光学系统能够在宏像素后面成更大的像，为主要像。

而且对下面的图形进行分析，这里可以看出来，在微透镜阵列焦距保持比较好的时候，成像阵列上有位置是没有成像的。这也是上面的图形中为什么有很多黑色小圆圈的原因。

分析可知，每个微透镜的所成宏像素的大小是保持不变的。即在成像阵列上，所成图像的大小都是一样的。

---

![1552703813293](images/1552703813293.png)

![1552703872897](images/1552703872897.png)

该方法可以快速计算出各个微透镜所对应宏像素的位置，有效的提取出各个宏像素，方便后续处理。

<font color=#ff0000>将各个宏像素中同一位置处的单元像素进行提取，我们就能够获得各个孔径所成的子图像。</font>按照子孔径的排列顺序排列出来，如图所示。

---

思考：上面红字中说同一位置处的单元像素进行提取，每个宏像素是对整个系统进行成像包含了所有的像点位置。这里的意思应该是将变形的图像进行整形。这里没有进行具体的说明，等后面有进一步了解的时候再来仔细看看。

下面说了，子通道图片的分辨率和微透镜数目相同。那么这里可能是将每个微透镜阵列中同一位置提取出来进行叠加。这样叠加成一副分辨率和微透镜数目一致的图片。

<font color=#ff0000>可知，还原出来的图片分辨率和微透镜数目一直，还原出来的图片数量也和微透镜的数目一致。</font>

进一步分析：思考如果将每个微透镜对应宏像素的中心位置进行提取，那么可以提取出来一个以微透镜阵列数目为分辨率的图形。再次考虑，由于透镜在近轴具有很好的稳定性，成像效果在近轴处不会出现很大的畸变。因此考虑对近轴的像素进行提取的方法可行。由于偏离轴太多的地方成像存在巨大的畸变，不宜进行图像的提取，<font color=#ff0000>因此能够成功还原出来的图片数量应该是比较少的。</font>

---

![1552704170204](images/1552704170204.png)

其中，各个<font color=#ff0000>子通道图片的分辨率与微透镜阵列数目相同</font>，从单个通道子图中发现，单通道子图还存在许多噪声。单通道子图是相机主光学系统的部分孔径所成的像，因此景深有所提高，然后由于在相同的曝光时间下，子孔径所成像的曝光量变小，因此会产生更多的噪声，信噪比相对于全空净成像有所降低。不同孔径图像的对比中，发现不同的视角图像之间存在不同的位置关系，从三维成像的角度来看，这是由于采集时，相机的孔径存在视差关系。将这些不同孔径成的子图像直接进行叠加，可以得到整个孔径在此时的成像结果。

---

思考：由于在提取图形的时候是系统利用图形中畸变比较小的成像结果来进行图像的还原的，因此图像具有很小的视角差，这个可能会在一定程度上影响到三维重建的结果。由于每个还原出来的图像都是对应不同的视角，从原来上来分析是对应是不同的点的，<font color=#ff0000>可以对图形进行插入的方式将图形的分辨率进行提高。</font>

---

![1552708818055](images/1552708818055.png)

由于此时的对焦关系不准确，目标物体此时无法正确的成像，从光场的角度来看就是不同孔径所成的像无法正确叠加。从光场的角度对这些子图像进行放大，平移和叠加，在得到理想成像面处的成像结果。如上图右边所示，物体显示清晰，所有的单通道图像准确叠加，信噪比相对于单通道有较大的提高。推测利用该方法可以在任意单通道子图像景深范围内重聚焦，通过一定的成像办法，就能实现相机景深的延长。

---

小结：对于每个单孔径来说，都是很小的，在单孔径下具有很好的景深效果，通俗来说就是不管物体的远近都能够得到很好的成像效果，因此单通道的图像肯定是清晰的。但是由于单孔径通光量的不足，导致此时的信噪比较低，对多孔径的图像进行叠加实现图像的还原。

---

## 2.3 聚焦型光场

斯坦福光场相机将微透镜阵列放置在主光学系统的像平面处，并且将探测器阵列放置在微透镜阵列的焦平面处。因此，每个微透镜阵列对应一个宏像素，微透镜阵列的个数决定了光场相机的空间分辨率，宏单元对应的探测器像素数代表了光场相机的角度分辨率。

每个微透镜的孔径越大，得到的图像分辨率就会越小，但是每个微透镜阵列的角度分辨率就越大。因此为获得高像素的斯坦福光场相机就需要缩小微透镜阵列的孔径。但是这个在前期的制造中很难实现。

Andrew提出聚焦型光场相机，通过改变微透镜阵列在相机内部空间位置，将角度分辨率和空间分辨率矛盾的决定因素由微透镜阵列的孔径参数转变为微透镜阵列的空间位置参数。该方法增加了相机在构建时参数选择的灵活性，通过合理的参数选择，能够适当的提高最终图像分辨率。

---

**采集原理**

矩阵光学理论，空间光线表示为$r(q,p)$，$q$和$p$在三维下是一个二维向量，分别代表了光线的位置信息和角度信息。和光场的四维理论是相同的，可以把光场表示$L=(q,p)$。由矩阵光学理论，光线在自由空间中传播了一段距离$t$，有

---

注意：这里的公式中的$t$，应该是直接在Z轴上的投影距离。比如$f$。

---

![1552879317713](images/1552879317713.png)

![1552879353460](images/1552879353460.png)

---

注意：在下图中可以看到直接将$d/f​$作为角度，这个近似只有在角度很小的时候才能够进行近似的。那么可以认为这里面都是认为$d​$在一定程度上是远远小于$f​$的。

---

![1552879485667](images/1552879485667.png)

---

文中光场推导存在一定的错误，正确的推导如下。最终结果是$d/f*(f*p,p)​$，这里是假设每根光线的强度一样，将光强全部按照中心光强（设置位置q为0）进行叠加，并以此得到的结果。

![1552899966377](images/1552899966377.png)

---

![1552879540836](images/1552879540836.png)

从方程中我们可以看到，每个宏像素下的像素对应一条入射到单个微透镜口径中心的光线，这条光线方向为$q/f​$。因此斯坦福光场相机对光场采集结果如图

![1552880181601](images/1552880181601.png)

在利用光场图片生成二维图像时候，直接将采集的光场信息按角度叠加在一起，一个微透镜对应一个像素，导致图像分辨率严重降低。

---

思考：上述这种说法是不对的，因为在进行微透镜阵列采集的时候，虽然一个微透镜对应一个像素，但是一个微透镜可以提取很多次像素，完全可以将分辨率进行重建。而且实际中的成像也不是平行光线进行输入的。

---

聚焦型光场相机利用微透镜阵列对主光学系统所成像进行二次成像，用探测器阵列记录成像结果。在对主光学像进行二次成像的过程中有实像模式和虚像模式两种方式。这里对实像模式进行介绍。

![1552880354022](images/1552880354022.png)

以单通道模型对这一类光场相机的采集原理进行分析，单通道简化模型如图

![1552881187237](images/1552881187237.png)

其中主光学系统像面光场为$L_a​$，探测器面光场为$L_b​$，相机内部光线会在光学系统的像面即$a​$面汇聚，然后透过微透镜阵列记录在探测器面$b​$上。

根据矩阵光学，中间传播过程的传递矩阵可以表示为

---

下面的公式实在不知道是怎么计算出来的

![1552881579768](images/1552881579768.png)

因此可以得到

![1552881599568](images/1552881599568.png)

那么探测器面上某个微透镜下$q​$位置处的像素对应强度为

![1552881669799](images/1552881669799.png)

我们假设在$p=q/b​$方向的$a​$平面的光场为常数，并且根据几何关系入射至$q​$点的光线角度在$d/a​$范围内。有

![1552881754276](images/1552881754276.png)

方程可知，微透镜将主透镜像平面$a​$上的图像放大了$b/a​$倍后成倒像记录在图像传感器上，因此他的空间分辨率是探测器分辨率的$b/a​$倍，并且只有$a/b​$视场范围内的图片不会与其他微透镜采集的图片重复。

具体每个微透镜记录光场信息在光场分布图中可以表示为下图，每个网格代表一个像元，所有蓝色网格代表同一微透镜下的所有像元。可以看出，每个探测器像元都能够探测到主光学系统像面$a$上一个坐标位置的像点，每个像元所以可以记录通过该像点$b/a$角度范围内的光线。整个微透镜可以记录$a$平面上范围为$d*a/b$内的光线。图片重建结果发现，每个微透镜阵列不仅只对应一个像素，具体还原像素数是与参数$a$和$b​$来同时决定。聚焦型光场相机能够非常有效的调节空间分辨率与角度分辨率之间的相互关系，在保证重聚焦能力的同时，又尽量提高相机的分辨率。

![1552961189972](images/1552961189972.png)

---

**算法原理**

微透镜阵列对像面进行二次成像，可以得到微透镜阵列对应的放大率为$a/b$。主光学系统到到探测器阵列间投影示意图如图所示，<font color=#ff0000>物体的不同部分被记录在不同微透镜阵列所对应的像元区域内</font>，分别记录主光学像面的部分信息。根据角度分辨率原理，每个微图像之间的不重复部分视场只有$b/a$。在图中对应为$M$，$d$为微透镜阵列的孔径大小。

![1552961632436](images/1552961632436.png)

![1552961643913](images/1552961643913.png)

将这些不重复的部分拼接起来就能够得到一张完整的二维图像，不重复部分的像素数$M$一般远远大于1个，因此聚焦型光场相机的像素数比斯坦福光场相机有比较大的提高。

---

思考：这里指的不重复部分的像素大于1，那么就是和斯坦福光场相机在拼接上进行比较。斯坦福光场相机，一个微透镜阵列对应一个像素值，而聚焦型光场相机能够对应一个不重复区域的像素值。

---

微透镜与其所对应的像元区域可以视为一个独立的光学系统，由于独立微透镜阵列所组成的光学系统景深远远长过主光学系统焦深，可以认为微透镜记录的微图像可以对整个相机的物空间都是清晰成像的。重聚焦时，新的聚焦面与微透镜所记录宏像素之间的放大关系就可以通过几何关系重新计算出来。同样我们能够计算出不重复部分所包含的像素大小，将他们拼接，能偶获得重聚焦在不同深度上的图片。获得更高于斯坦福光场相机的像素利用率，同时保证了一定的重聚焦能力。

---

**聚焦型相机仿真**

![1552962348973](images/1552962348973.png)



![1552962361585](images/1552962361585.png)

空间信息经过主光学系统成像在$a​$平面上，微透镜阵列又对$a​$平面进行成像，最终的微图像阵列被探测器所采集。图像阵列如下图所示，包含微图像个数$39*29​$，每个微图像都包含$40*40​$个像素。从光学结构发现，$b/a=0.25​$，那么每个微图像中不重复的图片大小为$10​$个像素，提取中间的是个像素，对图像进行重建。

![1552962565342](images/1552962565342.png)

![1552962579984](images/1552962579984.png)

该方法快速有效重建出准确对焦时的图片结果，成像视角上和主光学没有差别，但是像素数有所降低。由于该方法舍去了大部分信息，只保留宏像素中间部分进行图像重建，造成信息的严重浪费，也降低了结果图片的信噪比，最终结果中存在较大的噪声。

















































